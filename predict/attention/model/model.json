{"class_name": "Functional", "config": {"name": "Attention_model", "layers": [{"class_name": "InputLayer", "config": {"batch_input_shape": [null, null, 33], "dtype": "float32", "sparse": false, "ragged": false, "name": "input_1"}, "name": "input_1", "inbound_nodes": []}, {"class_name": "Attention", "config": {"name": "Attention", "trainable": true, "dtype": "float32", "causal": false, "dropout": 0.0, "use_scale": false}, "name": "Attention", "inbound_nodes": [[["input_1", 0, 0, {}], ["input_1", 0, 0, {}]]]}, {"class_name": "GlobalMaxPooling1D", "config": {"name": "global_max_pooling1d", "trainable": true, "dtype": "float32", "data_format": "channels_last"}, "name": "global_max_pooling1d", "inbound_nodes": [[["Attention", 0, 0, {}]]]}, {"class_name": "Dense", "config": {"name": "p1", "trainable": true, "dtype": "float32", "units": 21, "activation": "relu", "use_bias": true, "kernel_initializer": {"class_name": "GlorotUniform", "config": {"seed": null}}, "bias_initializer": {"class_name": "Zeros", "config": {}}, "kernel_regularizer": null, "bias_regularizer": null, "activity_regularizer": null, "kernel_constraint": null, "bias_constraint": null}, "name": "p1", "inbound_nodes": [[["global_max_pooling1d", 0, 0, {}]]]}], "input_layers": [["input_1", 0, 0]], "output_layers": [["p1", 0, 0]]}, "keras_version": "2.4.0", "backend": "tensorflow"}